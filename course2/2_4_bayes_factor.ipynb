{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-872c1627-d723-4622-aa3e-67c4d8dd30ba","deepnote_cell_type":"code"},"source":"# Start writing code here...","execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport arviz as az\nimport pymc3 as pm\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta","metadata":{"tags":[],"cell_id":"00001-e966ddd2-4865-494f-99cc-2e6e129057f5","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The “Bayesian way” to compare models is to compute the marginal likelihood of each model $p(y∣ M_k)$, i.e. the probability of the observed data y given the $M_k$ model. This quantity, the marginal likelihood, is just the normalizing constant of Bayes’ theorem. We can see this if we write Bayes’ theorem and make explicit the fact that all inferences are model-dependent. Given the data y, the parameters of interest $\\theta$ and let $M_k$ be one model out of K competing models.\n\n$$p(\\theta|y, M_k) = \\frac{p(y|\\theta, M_k) p(\\theta | M_k)}{p(y|M_k)}$$","metadata":{"tags":[],"cell_id":"00002-38e90939-8d8b-4064-b343-7168f976845d","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"az.style.use(\"arviz-darkgrid\")","metadata":{"tags":[],"cell_id":"00002-895f8946-11cc-4366-99d6-5c3ade074c35","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"tags":[],"cell_id":"00004-be5ae4a6-40c8-4c19-a89e-a788f651d384","deepnote_cell_type":"code"},"outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"fdc964fe-7cf0-4529-afbc-ad01a2d3689f","deepnote_execution_queue":[]}}