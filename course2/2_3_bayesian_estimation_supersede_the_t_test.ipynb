{"cells":[{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-ac483492-3744-4980-8ca2-124aef3b1d5f","deepnote_cell_type":"code"},"source":"# Start writing code here...","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bayesian Estimation Supersedes the T-test\n\n## The Problem\nSeveral statistical inference procedures involve the comparison of two groups. We may be interested in whether one group is larger than another, or simply different from the other. We require a statistical model for this because true differences are usually accompanied by measurement or stochastic noise that prevent us from drawing conclusions simply from differences calculated from the observed data.\n\nThe de facto standard for statistically comparing two (or more) samples is to use a statistical test. This involves expressing a null hypothesis, which typically claims that there is no difference between the groups, and using a chosen test statistic to determine whether the distribution of the observed data is plausible under the hypothesis. This rejection occurs when the calculated test statistic is higher than some pre-specified threshold value.\n\nUnfortunately, it is not easy to conduct hypothesis tests correctly, and their results are very easy to misinterpret. Setting up a statistical test involves several subjective choices (e.g. statistical test to use, null hypothesis to test, significance level) by the user that are rarely justified based on the problem or decision at hand, but rather, are usually based on traditional choices that are entirely arbitrary (Johnson 1999). The evidence that it provides to the user is indirect, incomplete, and typically overstates the evidence against the null hypothesis (Goodman 1999).\n\nA more informative and effective approach for comparing groups is one based on estimation rather than testing, and is driven by Bayesian probability rather than frequentist. That is, rather than testing whether two groups are different, we instead pursue an estimate of how different they are, which is fundamentally more informative. Moreover, we include an estimate of uncertainty associated with that difference which includes uncertainty due to our lack of knowledge of the model parameters (epistemic uncertainty) and uncertainty due to the inherent stochasticity of the system (aleatory uncertainty).\n\nTo illustrate how this Bayesian estimation approach works in practice, we will use a fictitious example from Kruschke (2012) concerning the evaluation of a clinical trial for drug evaluation. The trial aims to evaluate the efficacy of a “smart drug” that is supposed to increase intelligence by comparing IQ scores of individuals in a treatment arm (those receiving the drug) to those in a control arm (those recieving a placebo). There are 47 individuals and 42 individuals in the treatment and control arms, respectively.","metadata":{"tags":[],"cell_id":"00001-0524861c-ee8c-424a-a1c3-8c5083e4a7e9","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Toy Example","metadata":{"tags":[],"cell_id":"00002-53e59875-3d1c-48fb-a1dc-8671f7d62f05","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"drug = (101,100,102,104,102,97,105,105,98,101,100,123,105,103,100,95,102,106,\n        109,102,82,102,100,102,102,101,102,102,103,103,97,97,103,101,97,104,\n        96,103,124,101,101,100,101,101,104,100,101)\nplacebo = (99,101,100,101,102,100,97,101,104,101,102,102,100,105,88,101,100,\n           104,100,100,100,101,102,103,97,101,101,100,101,99,101,100,100,\n           101,100,99,101,100,102,99,100,99)\n\ny1 = np.array(drug)\ny2 = np.array(placebo)\n\n# What does np.r_ do (numpy)?\n# What it does is row-wise merging. https://stackoverflow.com/questions/30597869/what-does-np-r-do-numpy\ny = pd.DataFrame(dict(value = np.r_[y1, y2], \n                      group = np.r_[[\"drug\"] * len(drug), [\"placebo\"] * len(placebo)]))\n\n","metadata":{"tags":[],"cell_id":"00001-024fb0c7-2645-4f41-9cb9-a4a57ad1a004","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prior for group mean\nmu_m = y.value.mean()\nmu_s = y.value.std() * 2\n\n# Prior for group standard deviation\nsigma_low = 1\nsigma_high = 10\n\n# Set up prior for 2 groups\nwith pm.Model() as model:\n    # Normal prior for group mean k = 1,2\n    group1_mean = pm.Normal('group1_mean', mu = mu_m, sd = mu_s)\n    group2_mean = pm.Normal('group2_mean', mu = mu_m, sd = mu_s)\n    \n    # Uniform prior for group standard deviation\n    group1_std = pm.Uniform('group1_std', lower = sigma_low, upper = sigma_high)\n    group2_std = pm.Uniform('group2_std', lower = sigma_low, upper = sigma_high)\n    \n    # Exponential prior for degree of freedom\n    v = pm.Exponential('v_minus_one', 1/29) + 1\n    \n    # Transform the standard deviation to precision\n    lambda_1 = group1_std ** -2\n    lambda_2 = group2_std ** -2\n    \n    # Assign likelihood using T-distribution\n    # You can see that we use the same prior for both drug and placebo population\n    group1 = pm.StudentT(\"drug\", nu = v, mu = group1_mean, lam = lambda_1, observed = y1)\n    group2 = pm.StudentT(\"placebo\", nu = v, mu = group2_mean, lam = lambda_2, observed = y2)\n    \n    # Deterministic nodes for difference of means and standard deviations between two groups\n    diff_means = pm.Deterministic(\"difference of means\", group1_mean - group2_mean)\n    diff_stds = pm.Deterministic(\"difference of stds\", group1_std - group2_std)\n    effect_size = pm.Deterministic(\"effect size\", diff_means / np.sqrt((group1_std ** 2 + group2_std **2) / 2))\n    \n    # Fit the model and evaluate the output as trace\n    trace = pm.sample(2000, chains = 4, tune = 500)","metadata":{"tags":[],"cell_id":"00004-5d776ec8-1ec9-47b5-ac1f-6012d04bc39f","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The first step of visualization comes with using the plot_posterior function. \n# It takes the trace object and show the posterior mean, standard deviation and degree of freedom of the model\npm.plot_posterior(\n    trace,\n    var_names=[\"group1_mean\", \"group2_mean\", \"group1_std\", \"group2_std\", \"v_minus_one\"],\n    color=\"#87ceeb\",\n)","metadata":{"tags":[],"cell_id":"00005-ad2673eb-5a4d-4f6a-ab78-bb302f563bf8","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Looking at the group differences below, we can conclude that there are meaningful differences between the two groups for all three measures. For these comparisons, it is useful to use zero as a reference value (ref_val); providing this reference value yields cumulative probabilities for the posterior distribution on either side of the value. Thus, for the difference of means, at least 97% of the posterior probability are greater than zero, which suggests the group means are credibly different. The effect size and differences in standard deviation are similarly positive.","metadata":{"tags":[],"cell_id":"00006-2f7482a7-9b84-4f57-93fe-7efa9e707517","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# We move on to plot the posterior distribution of the difference in means, standard deviation and effect size of the two groups\npm.plot_posterior(trace,\n                  var_names = [\"difference of means\", \"difference of stds\", \"effect size\"],\n                  ref_val = 0,\n                  color = \"#87ceeb\")","metadata":{"tags":[],"cell_id":"00007-be063041-0374-4ac7-8bcd-9411e2483d7c","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When forestplot is called on a trace with more than one chain, it also plots the potential scale reduction parameter, which is used to reveal evidence for lack of convergence; values near one, as we have here, suggest that the model has converged.","metadata":{"tags":[],"cell_id":"00008-0a656bbf-4e06-44d7-98c1-c66014699876","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# Now we make a forest plot to see how much mean does the two groups differ\npm.forestplot(trace, \n          var_names = [\"group1_mean\", \"group2_mean\"])","metadata":{"tags":[],"cell_id":"00009-ce64c1be-e0a5-4106-be34-68549fbe43fc","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We also want to see how much spread difference occurred between the two groups by comparing standard deviation\npm.forestplot(trace, \n          var_names = [\"group1_std\", \"group2_std\"])","metadata":{"tags":[],"cell_id":"00010-60b6792d-2fff-470c-a19f-107db57ca762","deepnote_cell_type":"code"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Finally we look at the parameters in table-like view using pm.summary\npm.summary(trace, varnames = [\"difference of means\", \"difference of stds\", \"effect size\"])","metadata":{"tags":[],"cell_id":"00011-e608602f-7ae7-4f63-a9a6-153a83d4e08c","deepnote_cell_type":"code"},"outputs":[],"execution_count":null}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"a3247eaa-e97f-41e1-85cb-c1defc9f51a4","deepnote_execution_queue":[]}}