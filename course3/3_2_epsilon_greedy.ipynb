{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Epsilon Greedy Algorithm\n","\n","Welcome back! In this lecture we are going to discuss the multi-armed bandit problem and the first arm selection policy that is readily implementable in Python. To recap, a multi-armed bandits is a learning algorithm that allocates a fixed number of resources to a set of competing choices, with the purpose of learning the optimal resource allocation rule over time. Therefore, it's useful to establish a few parameters and principles in the bansit setting. First of all\n","\n","* You're presented with k distinct \"arms\" to choose from. An arm could define a piece of content for a pick"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We can implement a traditional epsilon greedy bandit strategy using the epsilon greedy policy definition.\n","\n","def epsilon_greedy_policy(df, arms, epsilon = 0.2, slate_size = 5, batch_size = 50):\n","    '''\n","    We take five arguments to iteratively update epsilon greedy policy to generate movie recommendations.\n","    \n","    The arguments are:\n","        df: dataframe. The dataset to apply the policy to\n","        arms: it is a list or an array of every eligible arm, such as ID, video, etc\n","        epsilon: it is a float that represents the proportion of timesteps where we explore random arms\n","        slate_size: it is an integer representing the number of recommendations to make at each step\n","        batch_size:\n","    \n","    '''\n","    \n","    # The first step is to use np.random.binomial to define the probability of explore as a binomial\n","    # distribution, with epsilon specifying the likelihood of drawing a 1, otherwise 0\n","    explore = np.random.binomial(1, epsilon)\n","    \n","    # There are two cases when we explore random recommendations. First, if the value fo explore is 1. \n","    # Second, if the recommender system has no data in history, that is, when we just initiate the process.\n","    \n","    # So if explore equals to 1 or if the first row of data is used\n","    if explore == 1 or df.shape[0] == 0:\n","        # we use np.random.choice to shuffle the items to choose a random set of recommendations. \n","        # Note that arms is an array of eligible arm to choose for recommendation,\n","        # and at this step, choose the recommendations equal to its slate_size without replacement.\n","        recs = np.random.choice(arms, size = (slate_size), replace = False)\n","        \n","    # Other than the two cases, the recommender system will exploit the arms that have performed\n","    # the best historically. We sort the arms by the proportion of people rated good and recommend \n","    # the items with the best performance so far!\n","    \n","    else:\n","        \n","        scores = df['column'].groupby('ID').agg({'target': ['mean', 'count']})\n","        \n","        scores.columns = ['mean', 'count']\n","        \n","        scores['column'] = scores.index\n","        \n","        score = scores.sort_values('mean', ascending = False)\n","        \n","        recs = scores.loc[scores.index[0:slate_size], 'column'].values\n","        \n","    return recs\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"deepnote_notebook_id":"d6da5887-a007-411b-8d30-78cbe61717cd"},"nbformat":4,"nbformat_minor":4}